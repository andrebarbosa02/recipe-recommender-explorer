{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAC Project 1 (SNA + RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "members = pd.read_csv('data/pp_members.csv')\n",
    "recipes = pd.read_csv('data/pp_recipes.csv')\n",
    "reviews = pd.read_csv('data/pp_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def ing_process(x, ing_or_quant):\n",
    "\n",
    "    try: \n",
    "        ing_list = ast.literal_eval(x)\n",
    "    except:\n",
    "        print(x)\n",
    "        return None    \n",
    "\n",
    "    try:\n",
    "        res = list(ing_list.values())[0]\n",
    "    except:\n",
    "        print(ing_list)\n",
    "        return None\n",
    "    \n",
    "    return [x[ing_or_quant] for x in res]\n",
    "\n",
    "recipes['ingredients_pp'] = recipes['ingredients'].apply(ing_process, args=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['ingredients_pp']\n",
    "recipes['ingredients_pp'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['quantities_pp'] = recipes['ingredients'].apply(ing_process, args=(1,))\n",
    "recipes['quantities_pp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['ingredients_pp'].apply(type).unique()\n",
    "\n",
    "recipes[recipes['ingredients_pp'].apply(type) == type(None)]\n",
    "\n",
    "recipes = recipes.drop(recipes[recipes['ingredients_pp'].apply(type) == type(None)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "from collections import defaultdict\n",
    "\n",
    "# Create edges for recipes, based on ingredients in common as weight\n",
    "def ing_freq_edge_weight(df,min_weight=0):\n",
    "    ingredients_freq = {}\n",
    "    # frequency of each ingredient save to a dict\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.iloc[i]['ingredients_pp'])):\n",
    "            if df.iloc[i]['ingredients_pp'][j] in ingredients_freq:\n",
    "                ingredients_freq[df.iloc[i]['ingredients_pp'][j]] += 1\n",
    "            else:\n",
    "                ingredients_freq[df.iloc[i]['ingredients_pp'][j]] = 1\n",
    "\n",
    "    print(\"ing freq\", ingredients_freq)\n",
    "    long_df = df.explode('ingredients_pp')\n",
    "    graph_structure = defaultdict(dict)\n",
    "\n",
    "    for ingredient, rows in long_df.groupby('ingredients_pp'):\n",
    "        # Get all unique pairs of recipes containing this ingredient\n",
    "        pairs = itertools.combinations(rows.index.unique(), 2)\n",
    "\n",
    "        # Calculate weight based on ingredient frequency\n",
    "        weight = 1 / ingredients_freq[ingredient]\n",
    "\n",
    "        # Update the graph structure with the weight for each pair\n",
    "        for a, b in pairs:\n",
    "            if b in graph_structure[a]:\n",
    "                graph_structure[a][b] += weight\n",
    "                graph_structure[b][a] += weight\n",
    "            else:\n",
    "                graph_structure[a][b] = weight\n",
    "                graph_structure[b][a] = weight\n",
    "\n",
    "    # Convert the graph structure to a list of tuples [(index1, index2, weight), ...]\n",
    "    index_pairs = [(a, b, graph_structure[a][b]) for a in graph_structure for b in graph_structure[a] if (a < b) and (graph_structure[a][b]>=min_weight)]\n",
    "\n",
    "    pairs_df = pd.DataFrame(index_pairs, columns=['from', 'to','weight'])\n",
    "    return pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 39)\n",
    "\n",
    "# get the top 1000 recipes with the most ratings\n",
    "top_recipes = recipes.sort_values(by='number_of_ratings', ascending=False)[0:10000]\n",
    "\n",
    "# get the top 10000 recipes with the most recent dates\n",
    "top_recent_recipes = recipes.sort_values(by='last_changed_date', ascending=False)[0:10000]\n",
    "\n",
    "top_recipes = top_recipes.sort_values(by='last_changed_date')\n",
    "top_recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
    "small_count_vectorizer = TfidfVectorizer(stop_words='english', max_features=40000)\n",
    "pln_top_reviews = top_recipes.dropna(subset=['description'])\n",
    "print(pln_top_reviews.shape)\n",
    "mixed_pln_top_reviews = pln_top_reviews.copy()\n",
    "mixed_pln_top_reviews['description_ingredients'] = mixed_pln_top_reviews.apply(lambda row: ''.join(row['description'] + ' ' + ' '.join(map(str,row['ingredients_pp']))), axis=1)\n",
    "# extract a dataframe(small_text_sample) from top_recipes that brings description and last_changed_date\n",
    "#small_text_sample = mixed_pln_top_reviews['description']\n",
    "mixed_pln_top_reviews = mixed_pln_top_reviews.dropna(subset=['ingredients_pp'])\n",
    "mixed_pln_top_reviews = mixed_pln_top_reviews.dropna(subset=['description_ingredients'])\n",
    "#small_text_sample = mixed_pln_top_reviews['description_ingredients']\n",
    "mixed_pln_top_reviews['ingredients_string'] = mixed_pln_top_reviews.apply(lambda row: ' '.join(map(str,row['ingredients_pp'])),axis=1)\n",
    "small_text_sample = mixed_pln_top_reviews['ingredients_string']\n",
    "#for item in small_text_sample:\n",
    "#    print(item)\n",
    "small_text_sample.index = mixed_pln_top_reviews['last_changed_date']\n",
    "len(small_text_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "sw = set(stopwords.words('english'))\n",
    "non_string_items = small_text_sample.apply(lambda x: not isinstance(x, str))\n",
    "non_string_indices = non_string_items[non_string_items].index\n",
    "\n",
    "# Print out the non-string items\n",
    "for index in non_string_indices:\n",
    "    print(f\"Index: {index}, Value: {small_text_sample[index]}\")\n",
    "\n",
    "# iterate over the pln_top_reviews['text'] to replace the characters\n",
    "small_text_sample = small_text_sample.apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "# to lower-case\n",
    "small_text_sample = small_text_sample.apply(lambda x: x.lower())\n",
    "# split into tokens, apply stemming and remove stop words\n",
    "small_text_sample = small_text_sample.apply(lambda x: ' '.join([ps.stem(w) for w in x.split() if w not in sw]))\n",
    "len(small_text_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "print(small_text_sample)\n",
    "\n",
    "small_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n",
    "n_topics = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_keys(topic_matrix):\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "def keys_to_counts(keys):\n",
    "    count_pairs = sorted(Counter(keys).items())\n",
    "    print(count_pairs)\n",
    "    categories = [pair[0] for pair in sorted(count_pairs)]\n",
    "    counts = [pair[1] for pair in sorted(count_pairs)]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    Returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order.\n",
    "    '''\n",
    "    top_words = []\n",
    "    n_topics = np.unique(keys).size  # Ensure you know the number of unique topics\n",
    "\n",
    "    for topic in range(n_topics):\n",
    "        # Initialize a zero vector of the same shape as a row in your document_term_matrix\n",
    "        temp_vector_sum = np.zeros((1, document_term_matrix.shape[1]))\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                # Increment by the row corresponding to the document associated with the topic\n",
    "                temp_vector_sum += document_term_matrix[i].toarray()  # convert sparse matrix row to dense\n",
    "\n",
    "        # Extract the indices of the top n words; these are the columns in the matrix\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:], 0)\n",
    "        \n",
    "        # Retrieve the actual words from the count_vectorizer\n",
    "        topic_words = [count_vectorizer.get_feature_names_out()[index] for index in top_n_word_indices]\n",
    "        top_words.append(\" \".join(topic_words))\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_topic_vectors(keys, two_dim_vectors):\n",
    "    '''\n",
    "    returns a list of centroid vectors from each predicted topic category\n",
    "    '''\n",
    "    mean_topic_vectors = []\n",
    "    for t in range(n_topics):\n",
    "        articles_in_that_topic = []\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == t:\n",
    "                #print(t, two_dim_vectors[i])\n",
    "                articles_in_that_topic.append(two_dim_vectors[i])    \n",
    "        print(articles_in_that_topic)\n",
    "        articles_in_that_topic = np.vstack(articles_in_that_topic)\n",
    "        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n",
    "        mean_topic_vectors.append(mean_article_in_that_topic)\n",
    "    return mean_topic_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\n",
    "colormap = colormap[:n_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
    "                                          random_state=0, verbose=0)\n",
    "lda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_topic_matrix)\n",
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)\n",
    "print(lda_keys)\n",
    "print(lda_counts)\n",
    "print(lda_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lda)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in range(len(top_3_words))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lda_categories, lda_counts)\n",
    "ax.set_xticks(lda_categories)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_title('LDA topic counts')\n",
    "ax.set_ylabel('Number of headlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\n",
    "tsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "lda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n",
    "print(len(lda_keys))\n",
    "print(len(tsne_lda_vectors))\n",
    "print(lda_mean_topic_vectors)\n",
    "\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), width=700, height=700)\n",
    "plot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n",
    "\n",
    "for t in range(n_topics):\n",
    "    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n",
    "                  text=top_3_words_lda[t], text_color=colormap[t])\n",
    "    plot.add_layout(label)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_topic_recipes_ids = [i for i in range(len(lda_keys)) if lda_keys[i] == lda_counts.index(max(lda_counts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = mixed_pln_top_reviews.iloc[largest_topic_recipes_ids]['new_recipe_id']\n",
    "print(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reviews_from_topic = reviews[reviews['recipe_id'].isin(recipes)]\n",
    "top_reviews_from_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all reviews associated with recipes in top_recipes\n",
    "pln_top_reviews = reviews[reviews['recipe_id'].isin(top_recipes['new_recipe_id'])]\n",
    "# get reviews with rating of 1.0\n",
    "#pln_top_reviews = pln_top_reviews[pln_top_reviews['rating'] == 1.0]\n",
    "pln_top_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only review_id, recipe_id, member_id, text, rating\n",
    "pln_top_reviews = pln_top_reviews[['review_id','recipe_id','member_id','text','rating']]\n",
    "pln_top_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is any NA in text\n",
    "pln_top_reviews[pln_top_reviews['text'].isna()]\n",
    "# drop NA\n",
    "pln_top_reviews = pln_top_reviews.dropna(subset=['text'])\n",
    "# print type of text\n",
    "pln_top_reviews['text'].apply(type).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "# iterate over the pln_top_reviews['text'] to replace the characters\n",
    "pln_top_reviews['text'] = pln_top_reviews['text'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "# to lower-case\n",
    "pln_top_reviews['text'] = pln_top_reviews['text'].apply(lambda x: x.lower())\n",
    "# split into tokens, apply stemming and remove stop words\n",
    "pln_top_reviews['text'] = pln_top_reviews['text'].apply(lambda x: ' '.join([ps.stem(w) for w in x.split() if w not in sw]))\n",
    "\n",
    "\n",
    "pln_top_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get size of the dataset\n",
    "pln_top_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a wordcloud from the text column\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#remove words from wordcloud\n",
    "words_to_remove = ['use', 'recip', 'made', 'make', 'thank', 'love', 'good', 'ad', 'hand']\n",
    "pln_top_reviews['text'] = pln_top_reviews['text'].apply(lambda x: ' '.join([w for w in x.split() if w not in words_to_remove]))\n",
    "\n",
    "wordcloud = WordCloud().generate(\" \".join(pln_top_reviews['text']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(pln_top_reviews['text']).toarray()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# get the representation vector of the row 124\n",
    "print(X[1])\n",
    "\n",
    "# get which columns on the row 124 have 1s\n",
    "print([vectorizer.get_feature_names_out()[i] for i in range(len(X[1])) if X[1][i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pln_top_reviews['rating']\n",
    "#create df with y\n",
    "#y = pd.DataFrame(y)\n",
    "\n",
    "# convert y in 0 or 1, it is 0 if the rating is below 4, it is 1 if it is equal or higher than 4\n",
    "y = y.apply(lambda x: 0 if x < 4 else 1)\n",
    "\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = SVC()\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nLabel distribution in the test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(precision_score(y_test,y_pred))\n",
    "print(recall_score(y_test,y_pred))\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "rev = input(\"Enter review: \")\n",
    "\n",
    "rev = re.sub('[^a-zA-Z]', ' ', rev)\n",
    "rev = rev.lower()\n",
    "rev = ' '.join([ps.stem(w) for w in rev.split() if w not in sw])\n",
    "\n",
    "V = vectorizer.transform([rev]).toarray()\n",
    "\n",
    "print(rev)\n",
    "print(V.shape)\n",
    "print(V)\n",
    "print([vectorizer.get_feature_names_out()[i] for i in range(len(V[0])) if V[0][i] == 1])\n",
    "\n",
    "if(clf.predict(V) == [1]):\n",
    "    print('positive review (+)')\n",
    "else:\n",
    "    print('negative review (-)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for rev in pln_top_reviews['text']:\n",
    "    y_pred.append(1 if analyzer.polarity_scores(rev)['compound'] > 0 else 0)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y, y_pred))\n",
    "print('Precision: ', precision_score(y, y_pred))\n",
    "print('Recall: ', recall_score(y, y_pred))\n",
    "print('F1: ', f1_score(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
